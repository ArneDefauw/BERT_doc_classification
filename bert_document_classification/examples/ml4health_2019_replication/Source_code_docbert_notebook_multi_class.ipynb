{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:99% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from base64 import b64encode\n",
    "from base64 import b64decode\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (60,60)\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "# make the Jupyter notebook use the full screen width\n",
    "display(HTML(\"<style>.container { width:99% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg_resources import resource_exists, resource_listdir, resource_string, resource_stream,resource_filename\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy\n",
    "import collections\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "#from pytorch_transformers.tokenization_bert import BertTokenizer\n",
    "#from pytorch_transformers.modeling_utils import  CONFIG_NAME\n",
    "#from pytorch_transformers.modeling_bert import BertPreTrainedModel, BertConfig, BertModel\n",
    "\n",
    "from transformers import CONFIG_NAME, WEIGHTS_NAME\n",
    "from transformers.tokenization_bert import BertTokenizer\n",
    "#from transformers.modeling_utils import  CONFIG_NAME\n",
    "from transformers.modeling_bert import BertPreTrainedModel, BertConfig, BertModel\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn import LSTM\n",
    "import torch, math, logging, os\n",
    "import sys, os, time, socket\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels='PAST SMOKER, CURRENT SMOKER, NON-SMOKER, UNKNOWN'\n",
    "device='cuda:1'\n",
    "batch_size=5  #batch size of lstm\n",
    "\n",
    "\n",
    "bert_model_path= \\\n",
    "'/notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/'\n",
    "#bert_model_path='bert-base-uncased'\n",
    "bert_batch_size=7\n",
    "\n",
    "#parameters for optimizer\n",
    "weight_decay=0\n",
    "learning_rate=6e-5\n",
    "\n",
    "model_storage_directory='results_multi_class'\n",
    "\n",
    "\n",
    "labels = [x for x in labels.split(', ')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set run specific envirorment configurations\n",
    "timestamp = time.strftime(\"run_%Y_%m_%d_%H_%M_%S\") + \"_{machine}\".format(machine=socket.gethostname())\n",
    "model_directory = os.path.join(model_storage_directory, timestamp) #directory\n",
    "os.makedirs(model_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling logging configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger()\n",
    "log.handlers.clear()\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "fh = logging.FileHandler(os.path.join(model_directory, \"log.txt\"))\n",
    "fh.setLevel(logging.INFO)\n",
    "fh.setFormatter(formatter)\n",
    "log.addHandler(fh)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "log.setLevel(logging.INFO)\n",
    "log.addHandler(ch)\n",
    "#log.info(p.format_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_n2c2_2006(partition='train'):\n",
    "    \"\"\"\n",
    "    Yields a generator of id, doc, label tuples.\n",
    "    :param partition:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert partition in ['train', 'test']\n",
    "\n",
    "    if partition == 'train':\n",
    "        with open(\"data/smokers_surrogate_%s_all_version2.xml\" % partition) as raw:\n",
    "            file = raw.read().strip()\n",
    "        \n",
    "    elif partition == 'test':\n",
    "        with open(\"data/smokers_surrogate_%s_all_groundtruth_version2.xml\" % partition) as raw:\n",
    "            file = raw.read().strip()   \n",
    "        \n",
    "    # file = resource_string('clinical_data', 'phenotyping/n2c2_2006/smokers_surrogate_%s_all_version2.xml' % partition).decode('utf-8').strip()\n",
    "    root = ET.fromstring(file)\n",
    "    ids = []\n",
    "    notes = []\n",
    "    labels = []\n",
    "    documents = root.findall(\"./RECORD\")\n",
    "    for document in documents:\n",
    "        ids.append(document.attrib['ID'])\n",
    "        notes.append(document.findall('./TEXT')[0].text)\n",
    "        labels.append(document.findall('./SMOKING')[0].attrib['STATUS'])\n",
    "\n",
    "    for id, note, label in zip(ids,notes,labels):\n",
    "        yield (id,note,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data:\n",
    "\n",
    "#now treat it as a multi-class problem:\n",
    "\n",
    "train, dev = load_n2c2_2006(partition='train'), load_n2c2_2006(partition='test')\n",
    "\n",
    "train_documents, train_labels = [],[]\n",
    "for _, text, status in train:\n",
    "    if status in labels:\n",
    "        train_documents.append(text)\n",
    "        for idx, name in enumerate(labels):\n",
    "            if name == status:\n",
    "                label = idx\n",
    "        train_labels.append(label)\n",
    "\n",
    "dev_documents, dev_labels = [],[]\n",
    "for _, text, status in dev:\n",
    "    if status in labels:\n",
    "        dev_documents.append(text)\n",
    "        for idx, name in enumerate(labels):\n",
    "            if name == status:\n",
    "                label = idx\n",
    "        dev_labels.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentBertLSTM(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    BERT output over document in LSTM\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert_model_config: BertConfig):\n",
    "        super(DocumentBertLSTM, self).__init__(bert_model_config)\n",
    "        self.bert = BertModel(bert_model_config)\n",
    "        self.bert_batch_size= self.bert.config.bert_batch_size\n",
    "        self.dropout = nn.Dropout(p=bert_model_config.hidden_dropout_prob)\n",
    "        self.lstm = LSTM(bert_model_config.hidden_size,bert_model_config.hidden_size, )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=bert_model_config.hidden_dropout_prob),\n",
    "            nn.Linear(bert_model_config.hidden_size, bert_model_config.num_labels),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.init_weights()        \n",
    "\n",
    "    #input_ids, token_type_ids, attention_masks\n",
    "    def forward(self, document_batch: torch.Tensor, document_sequence_lengths: list, device='cuda'):\n",
    "\n",
    "        #contains all BERT sequences\n",
    "        #bert should output a (batch_size, num_sequences, bert_hidden_size)\n",
    "        bert_output = torch.zeros(size=(document_batch.shape[0],\n",
    "                                              min(document_batch.shape[1],self.bert_batch_size),\n",
    "                                              self.bert.config.hidden_size), dtype=torch.float, device=device)\n",
    "\n",
    "        #only pass through bert_batch_size numbers of inputs into bert.\n",
    "        #this means that we are possibly cutting off the last part of documents.\n",
    "        #use_grad = not freeze_bert\n",
    "    \n",
    "        \n",
    "        #with torch.set_grad_enabled( not freeze_bert ): #bert is freezed by default...\n",
    "        #with torch.no_grad(): #equivalent\n",
    "        \n",
    "        for doc_id in range(document_batch.shape[0]):\n",
    "            bert_output[doc_id][:self.bert_batch_size] = self.dropout(self.bert(document_batch[doc_id][:self.bert_batch_size,0],\n",
    "                                            token_type_ids=document_batch[doc_id][:self.bert_batch_size,1],\n",
    "                                            attention_mask=document_batch[doc_id][:self.bert_batch_size,2])[1])\n",
    "\n",
    "        #lstm expects a ( num_sequences, batch_size (i.e. number of documents) , bert_hidden_size )\n",
    "        self.lstm.flatten_parameters()\n",
    "        output, (_, _) = self.lstm(bert_output.permute(1,0,2))\n",
    "        \n",
    "        #print(bert_output.requires_grad)\n",
    "        #print(output.requires_grad)\n",
    "\n",
    "        last_layer = output[-1]\n",
    "        #print(\"Last LSTM layer shape:\",last_layer.shape)\n",
    "\n",
    "        prediction = self.classifier(last_layer)\n",
    "        #print(\"Prediction Shape\", prediction.shape)\n",
    "        assert prediction.shape[0] == document_batch.shape[0]\n",
    "        return prediction\n",
    "    \n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/\n",
      "BertConfig {\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bert_batch_size\": 7,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists( bert_model_path  ):\n",
    "    if os.path.exists(os.path.join(bert_model_path, CONFIG_NAME)):\n",
    "        print( f\"loading {bert_model_path}\" )\n",
    "        config = BertConfig.from_json_file(os.path.join(bert_model_path, CONFIG_NAME))\n",
    "    elif os.path.exists(os.path.join(bert_model_path, 'bert_config.json')):\n",
    "        print( f\"loading {bert_model_path}\" )\n",
    "        config = BertConfig.from_json_file(os.path.join(bert_model_path, 'bert_config.json'))\n",
    "    else:\n",
    "        raise ValueError(\"Cannot find a configuration for the BERT based model you are attempting to load.\")\n",
    "else:\n",
    "    config = BertConfig.from_pretrained(bert_model_path )\n",
    "\n",
    "\n",
    "config.__setattr__( 'num_labels', len( labels ) )\n",
    "config.__setattr__( 'bert_batch_size', bert_batch_size )\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Helper function for encoding documents (tokenization of sentences, word2id), and saving checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_documents(documents: list, tokenizer: BertTokenizer, max_input_length=512):\n",
    "    \"\"\"\n",
    "    Returns a len(documents) * max_sequences_per_document * 3 * 512 tensor where len(documents) is the batch\n",
    "    dimension and the others encode bert input.\n",
    "\n",
    "    This is the input to any of the document bert architectures.\n",
    "\n",
    "    :param documents: a list of text documents\n",
    "    :param tokenizer: the sentence piece bert tokenizer\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tokenized_documents = [tokenizer.tokenize(document)[:10200] for document in documents]  #added by AD (only take first 10200 tokens of each documents as input)\n",
    "    max_sequences_per_document = math.ceil(max(len(x)/(max_input_length-2) for x in tokenized_documents))\n",
    "    assert max_sequences_per_document <= 20, \"Your document is to large, arbitrary size when writing\"\n",
    "\n",
    "    output = torch.zeros(size=(len(documents), max_sequences_per_document, 3, 512), dtype=torch.long)\n",
    "    document_seq_lengths = [] #number of sequence generated per document\n",
    "    #Need to use 510 to account for 2 padding tokens\n",
    "    for doc_index, tokenized_document in enumerate(tokenized_documents):\n",
    "        max_seq_index = 0\n",
    "        for seq_index, i in enumerate(range(0, len(tokenized_document), (max_input_length-2))):\n",
    "            raw_tokens = tokenized_document[i:i+(max_input_length-2)]\n",
    "            tokens = []\n",
    "            input_type_ids = []\n",
    "\n",
    "            tokens.append(\"[CLS]\")\n",
    "            input_type_ids.append(0)\n",
    "            for token in raw_tokens:\n",
    "                tokens.append(token)\n",
    "                input_type_ids.append(0)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            input_type_ids.append(0)\n",
    "\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            attention_masks = [1] * len(input_ids)\n",
    "\n",
    "            while len(input_ids) < max_input_length:\n",
    "                input_ids.append(0)\n",
    "                input_type_ids.append(0)\n",
    "                attention_masks.append(0)\n",
    "\n",
    "            assert len(input_ids) == 512 and len(attention_masks) == 512 and len(input_type_ids) == 512\n",
    "\n",
    "            #we are ready to rumble\n",
    "            output[doc_index][seq_index] = torch.cat((torch.LongTensor(input_ids).unsqueeze(0),\n",
    "                                                           torch.LongTensor(input_type_ids).unsqueeze(0),\n",
    "                                                           torch.LongTensor(attention_masks).unsqueeze(0)),\n",
    "                                                          dim=0)\n",
    "            max_seq_index = seq_index\n",
    "        document_seq_lengths.append(max_seq_index+1)\n",
    "    return output, torch.LongTensor(document_seq_lengths)\n",
    "\n",
    "#helper function to save checkpoints\n",
    "\n",
    "def save_checkpoint( model: DocumentBertLSTM, tokenizer: BertTokenizer , checkpoint_path: str):\n",
    "    \"\"\"\n",
    "    Saves an instance of the current model to the specified path.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        os.mkdir(checkpoint_path)\n",
    "    else:\n",
    "        raise ValueError(\"Attempting to save checkpoint to an existing directory\")\n",
    "    log.info(\"Saving checkpoint: %s\" % checkpoint_path )\n",
    "\n",
    "    #save finetune parameters\n",
    "    net = model\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        net = model.module\n",
    "    torch.save(net.state_dict(), os.path.join(checkpoint_path, WEIGHTS_NAME))\n",
    "    #save configurations\n",
    "    net.config.to_json_file(os.path.join(checkpoint_path, CONFIG_NAME))\n",
    "    #save exact vocabulary utilized\n",
    "    tokenizer.save_vocabulary(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Train and predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( model: DocumentBertLSTM , optimizer: torch.optim.Adam , tokenizer: BertTokenizer, train: tuple, dev: tuple, batch_size: int , output_path:str, labels:list, epochs=10 , device='cuda:0'  ):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    train_documents, train_labels = train  #train is tuple\n",
    "    dev_documents, dev_labels = dev\n",
    "    \n",
    "    document_representations, document_sequence_lengths = encode_documents(train_documents, bert_tokenizer)\n",
    "\n",
    "    correct_output = torch.LongTensor(train_labels)\n",
    "    \n",
    "    assert document_representations.shape[0] == correct_output.shape[0]\n",
    "    \n",
    "    #if torch.cuda.device_count()>1:\n",
    "        #model=torch.nn.DataParallel( model )\n",
    "    \n",
    "    model.to( device=device )\n",
    "    \n",
    "    #get the loss function\n",
    "    \n",
    "    #loss_weight = ((correct_output.shape[0] / torch.sum(correct_output, dim=0))-1).to(device=device)\n",
    "    #loss_function = torch.nn.BCEWithLogitsLoss(pos_weight=loss_weight)\n",
    "    \n",
    "    distribution=[collections.Counter( correct_output.numpy() )[ i  ] for i in np.unique(correct_output.numpy()) ]\n",
    "    loss_weight = (correct_output.shape[0]  /torch.Tensor( distribution ) -1).to(device=device)\n",
    "    loss_function=torch.nn.CrossEntropyLoss(weight=loss_weight)  #now it is multi-class\n",
    "    \n",
    "    for epoch in range( 1, epochs+1  ):\n",
    "        # shuffle\n",
    "        permutation = torch.randperm(document_representations.shape[0])\n",
    "        document_representations = document_representations[permutation]\n",
    "        document_sequence_lengths = document_sequence_lengths[permutation]\n",
    "        correct_output = correct_output[permutation]\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        for i in range(0, document_representations.shape[0], batch_size  ):\n",
    "\n",
    "            batch_document_tensors = document_representations[i:i + batch_size  ].to(device )\n",
    "            batch_document_sequence_lengths= document_sequence_lengths[i:i+ batch_size  ]\n",
    "            #self.log.info(batch_document_tensors.shape)\n",
    "            batch_predictions=model( batch_document_tensors, batch_document_sequence_lengths, device=device )  #we freeze bert\n",
    "            batch_correct_output = correct_output[i:i + batch_size ].to( device=device )\n",
    "            loss = loss_function(batch_predictions, batch_correct_output)\n",
    "            epoch_loss += float(loss.item())\n",
    "            #self.log.info(batch_predictions)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss /= int(document_representations.shape[0] / batch_size )  # divide by number of batches per epoch\n",
    "\n",
    "        #if 'use_tensorboard' in self.args and self.args['use_tensorboard']:\n",
    "        #    self.tensorboard_writer.add_scalar('Loss/Train', epoch_loss, self.epoch)\n",
    "\n",
    "        log.info('Epoch %i Completed: %f' % (epoch, epoch_loss))\n",
    "    \n",
    "        if epoch % 250 == 0:\n",
    "            save_checkpoint( model, tokenizer, os.path.join( output_path , \"checkpoint_%s\" % epoch ) )\n",
    "\n",
    "        # evaluate on development data\n",
    "        if epoch % 10 == 0:\n",
    "            predict( model, tokenizer, (dev_documents, dev_labels), batch_size , epoch, output_path, labels, device=device )\n",
    "                \n",
    "        #make sure that it is back on train...\n",
    "        \n",
    "\n",
    "def predict( model: DocumentBertLSTM, tokenizer: BertTokenizer, data, batch_size:int, epoch:int, output_path:str, labels:list, threshold=0, device='cuda:0' ):\n",
    "    \"\"\"\n",
    "    A tuple containing\n",
    "    :param data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    document_representations = None\n",
    "    document_sequence_lengths = None\n",
    "    correct_output = None\n",
    "    if isinstance(data, list):\n",
    "        document_representations, document_sequence_lengths = encode_documents(data, tokenizer)\n",
    "    if isinstance(data, tuple) and len(data) == 2:\n",
    "        log.info('Evaluating on Epoch %i' % (epoch))\n",
    "        document_representations, document_sequence_lengths = encode_documents(data[0], tokenizer)\n",
    "        #correct_output = torch.FloatTensor(data[1]).transpose(0,1)\n",
    "        correct_output = torch.LongTensor(data[1])\n",
    "        assert labels is not None\n",
    "\n",
    "    model.to(device=device )\n",
    "    \n",
    "    #put in eval mode:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = torch.empty((document_representations.shape[0], len(labels )  ))\n",
    "        for i in range(0, document_representations.shape[0], batch_size  ):\n",
    "            batch_document_tensors = document_representations[i:i + batch_size  ].to(device=device)\n",
    "            batch_document_sequence_lengths= document_sequence_lengths[i:i+batch_size  ]\n",
    "\n",
    "            prediction = model(batch_document_tensors, batch_document_sequence_lengths,device=device  )\n",
    "            predictions[i:i + batch_size  ] = prediction\n",
    "\n",
    "    \n",
    "    predictions_labels= torch.argmax(predictions, dim=1)  #get the predicted labels \n",
    "    \n",
    "    if isinstance(data, tuple) and len(data) == 2:\n",
    "        log.info( metrics.classification_report(   dev_labels, predictions_labels, target_names=labels     ) )\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return predictions_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model name '/notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "Didn't find file /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/added_tokens.json. We won't load it.\n",
      "Didn't find file /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/special_tokens_map.json. We won't load it.\n",
      "Didn't find file /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/tokenizer_config.json. We won't load it.\n",
      "loading file /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading weights file /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/pytorch_model.bin\n",
      "Weights of DocumentBertLSTM not initialized from pretrained model: ['lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'classifier.1.weight', 'classifier.1.bias']\n",
      "Weights from pretrained model not used in DocumentBertLSTM: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "Epoch 1 Completed: 1.418986\n",
      "Epoch 2 Completed: 1.416015\n",
      "Epoch 3 Completed: 1.396039\n",
      "Epoch 4 Completed: 1.373261\n",
      "Epoch 5 Completed: 1.353610\n",
      "Epoch 6 Completed: 1.354475\n",
      "Epoch 7 Completed: 1.336385\n",
      "Epoch 8 Completed: 1.338653\n",
      "Epoch 9 Completed: 1.327515\n",
      "Epoch 10 Completed: 1.329461\n",
      "Evaluating on Epoch 10\n",
      "/miniconda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "   PAST SMOKER       0.00      0.00      0.00        11\n",
      "CURRENT SMOKER       0.00      0.00      0.00        11\n",
      "    NON-SMOKER       0.26      0.88      0.41        16\n",
      "       UNKNOWN       0.85      0.65      0.74        63\n",
      "\n",
      "     micro avg       0.54      0.54      0.54       101\n",
      "     macro avg       0.28      0.38      0.29       101\n",
      "  weighted avg       0.57      0.54      0.53       101\n",
      "\n",
      "Epoch 11 Completed: 1.333016\n",
      "Epoch 12 Completed: 1.319821\n",
      "Epoch 13 Completed: 1.312172\n",
      "Epoch 14 Completed: 1.321042\n",
      "Epoch 15 Completed: 1.314077\n",
      "Epoch 16 Completed: 1.323641\n",
      "Epoch 17 Completed: 1.298860\n",
      "Epoch 18 Completed: 1.303195\n",
      "Epoch 19 Completed: 1.298923\n",
      "Epoch 20 Completed: 1.289002\n",
      "Evaluating on Epoch 20\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "   PAST SMOKER       0.00      0.00      0.00        11\n",
      "CURRENT SMOKER       0.00      0.00      0.00        11\n",
      "    NON-SMOKER       0.26      0.88      0.41        16\n",
      "       UNKNOWN       0.85      0.65      0.74        63\n",
      "\n",
      "     micro avg       0.54      0.54      0.54       101\n",
      "     macro avg       0.28      0.38      0.29       101\n",
      "  weighted avg       0.57      0.54      0.53       101\n",
      "\n",
      "Epoch 21 Completed: 1.314450\n",
      "Epoch 22 Completed: 1.305117\n",
      "Epoch 23 Completed: 1.312413\n",
      "Epoch 24 Completed: 1.286910\n",
      "Epoch 25 Completed: 1.308947\n",
      "Epoch 26 Completed: 1.298969\n",
      "Epoch 27 Completed: 1.270062\n",
      "Epoch 28 Completed: 1.291564\n",
      "Epoch 29 Completed: 1.292876\n",
      "Epoch 30 Completed: 1.293881\n",
      "Evaluating on Epoch 30\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "   PAST SMOKER       0.00      0.00      0.00        11\n",
      "CURRENT SMOKER       0.00      0.00      0.00        11\n",
      "    NON-SMOKER       0.26      0.88      0.41        16\n",
      "       UNKNOWN       0.85      0.65      0.74        63\n",
      "\n",
      "     micro avg       0.54      0.54      0.54       101\n",
      "     macro avg       0.28      0.38      0.29       101\n",
      "  weighted avg       0.57      0.54      0.53       101\n",
      "\n",
      "Epoch 31 Completed: 1.318083\n",
      "Epoch 32 Completed: 1.286322\n",
      "Epoch 33 Completed: 1.300853\n",
      "Epoch 34 Completed: 1.307017\n",
      "Epoch 35 Completed: 1.278676\n",
      "Epoch 36 Completed: 1.293809\n",
      "Epoch 37 Completed: 1.289276\n",
      "Epoch 38 Completed: 1.283169\n",
      "Epoch 39 Completed: 1.289499\n",
      "Epoch 40 Completed: 1.272783\n",
      "Evaluating on Epoch 40\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "   PAST SMOKER       0.00      0.00      0.00        11\n",
      "CURRENT SMOKER       0.00      0.00      0.00        11\n",
      "    NON-SMOKER       0.27      0.94      0.42        16\n",
      "       UNKNOWN       0.85      0.56      0.67        63\n",
      "\n",
      "     micro avg       0.50      0.50      0.50       101\n",
      "     macro avg       0.28      0.37      0.27       101\n",
      "  weighted avg       0.57      0.50      0.49       101\n",
      "\n",
      "Epoch 41 Completed: 1.260361\n",
      "Epoch 42 Completed: 1.253116\n",
      "Epoch 43 Completed: 1.289856\n",
      "Epoch 44 Completed: 1.275384\n",
      "Epoch 45 Completed: 1.288460\n",
      "Epoch 46 Completed: 1.258705\n",
      "Epoch 47 Completed: 1.271418\n",
      "Epoch 48 Completed: 1.258751\n",
      "Epoch 49 Completed: 1.252731\n",
      "Epoch 50 Completed: 1.258388\n",
      "Evaluating on Epoch 50\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "   PAST SMOKER       0.07      0.09      0.08        11\n",
      "CURRENT SMOKER       0.29      0.18      0.22        11\n",
      "    NON-SMOKER       0.25      0.44      0.32        16\n",
      "       UNKNOWN       0.83      0.68      0.75        63\n",
      "\n",
      "     micro avg       0.52      0.52      0.52       101\n",
      "     macro avg       0.36      0.35      0.34       101\n",
      "  weighted avg       0.59      0.52      0.55       101\n",
      "\n",
      "Epoch 51 Completed: 1.243848\n",
      "Epoch 52 Completed: 1.228996\n",
      "Epoch 53 Completed: 1.214803\n",
      "Epoch 54 Completed: 1.241102\n",
      "Epoch 55 Completed: 1.232464\n",
      "Epoch 56 Completed: 1.242323\n",
      "Epoch 57 Completed: 1.232813\n",
      "Epoch 58 Completed: 1.232516\n",
      "Epoch 59 Completed: 1.244239\n",
      "Epoch 60 Completed: 1.204492\n",
      "Evaluating on Epoch 60\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "   PAST SMOKER       0.12      0.18      0.15        11\n",
      "CURRENT SMOKER       0.00      0.00      0.00        11\n",
      "    NON-SMOKER       0.25      0.75      0.38        16\n",
      "       UNKNOWN       0.85      0.44      0.58        63\n",
      "\n",
      "     micro avg       0.42      0.42      0.42       101\n",
      "     macro avg       0.31      0.34      0.28       101\n",
      "  weighted avg       0.58      0.42      0.44       101\n",
      "\n",
      "Epoch 61 Completed: 1.241721\n",
      "Epoch 62 Completed: 1.195040\n",
      "Epoch 63 Completed: 1.224176\n",
      "Epoch 64 Completed: 1.240056\n",
      "Epoch 65 Completed: 1.240606\n",
      "Epoch 66 Completed: 1.178855\n",
      "Epoch 67 Completed: 1.224864\n",
      "Epoch 68 Completed: 1.215352\n",
      "Epoch 69 Completed: 1.210444\n",
      "Epoch 70 Completed: 1.216196\n",
      "Evaluating on Epoch 70\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "   PAST SMOKER       0.20      0.27      0.23        11\n",
      "CURRENT SMOKER       0.00      0.00      0.00        11\n",
      "    NON-SMOKER       0.31      0.69      0.42        16\n",
      "       UNKNOWN       0.84      0.60      0.70        63\n",
      "\n",
      "     micro avg       0.51      0.51      0.51       101\n",
      "     macro avg       0.34      0.39      0.34       101\n",
      "  weighted avg       0.60      0.51      0.53       101\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71 Completed: 1.182634\n",
      "Epoch 72 Completed: 1.213332\n",
      "Epoch 73 Completed: 1.214447\n",
      "Epoch 74 Completed: 1.176549\n",
      "Epoch 75 Completed: 1.199338\n",
      "Epoch 76 Completed: 1.218979\n",
      "Epoch 77 Completed: 1.201413\n",
      "Epoch 78 Completed: 1.222730\n",
      "Epoch 79 Completed: 1.180940\n",
      "Epoch 80 Completed: 1.235994\n",
      "Evaluating on Epoch 80\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "   PAST SMOKER       0.13      0.27      0.18        11\n",
      "CURRENT SMOKER       0.00      0.00      0.00        11\n",
      "    NON-SMOKER       0.30      0.44      0.36        16\n",
      "       UNKNOWN       0.85      0.65      0.74        63\n",
      "\n",
      "     micro avg       0.50      0.50      0.50       101\n",
      "     macro avg       0.32      0.34      0.32       101\n",
      "  weighted avg       0.60      0.50      0.54       101\n",
      "\n",
      "Epoch 81 Completed: 1.204246\n",
      "Epoch 82 Completed: 1.214081\n",
      "Epoch 83 Completed: 1.169425\n",
      "Epoch 84 Completed: 1.202329\n",
      "Epoch 85 Completed: 1.147138\n",
      "Epoch 86 Completed: 1.208465\n",
      "Epoch 87 Completed: 1.175348\n",
      "Epoch 88 Completed: 1.161439\n",
      "Epoch 89 Completed: 1.195538\n",
      "Epoch 90 Completed: 1.155083\n",
      "Evaluating on Epoch 90\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "   PAST SMOKER       0.11      0.18      0.14        11\n",
      "CURRENT SMOKER       0.00      0.00      0.00        11\n",
      "    NON-SMOKER       0.26      0.62      0.36        16\n",
      "       UNKNOWN       0.87      0.52      0.65        63\n",
      "\n",
      "     micro avg       0.45      0.45      0.45       101\n",
      "     macro avg       0.31      0.33      0.29       101\n",
      "  weighted avg       0.59      0.45      0.48       101\n",
      "\n",
      "Epoch 91 Completed: 1.196406\n",
      "Epoch 92 Completed: 1.169716\n",
      "Epoch 93 Completed: 1.165344\n",
      "Epoch 94 Completed: 1.179098\n",
      "Epoch 95 Completed: 1.194380\n",
      "Epoch 96 Completed: 1.128478\n",
      "Epoch 97 Completed: 1.145664\n",
      "Epoch 98 Completed: 1.154261\n",
      "Epoch 99 Completed: 1.139009\n",
      "Epoch 100 Completed: 1.119497\n",
      "Evaluating on Epoch 100\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "   PAST SMOKER       0.11      0.27      0.15        11\n",
      "CURRENT SMOKER       0.00      0.00      0.00        11\n",
      "    NON-SMOKER       0.23      0.38      0.29        16\n",
      "       UNKNOWN       0.85      0.54      0.66        63\n",
      "\n",
      "     micro avg       0.43      0.43      0.43       101\n",
      "     macro avg       0.30      0.30      0.27       101\n",
      "  weighted avg       0.58      0.43      0.47       101\n",
      "\n",
      "Epoch 101 Completed: 1.155554\n",
      "Epoch 102 Completed: 1.112846\n",
      "Epoch 103 Completed: 1.171507\n",
      "Epoch 104 Completed: 1.107216\n",
      "Epoch 105 Completed: 1.145788\n",
      "Epoch 106 Completed: 1.126670\n",
      "Epoch 107 Completed: 1.196811\n",
      "Epoch 108 Completed: 1.111065\n",
      "Epoch 109 Completed: 1.118127\n",
      "Epoch 110 Completed: 1.138294\n",
      "Evaluating on Epoch 110\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "   PAST SMOKER       0.09      0.18      0.12        11\n",
      "CURRENT SMOKER       0.14      0.18      0.16        11\n",
      "    NON-SMOKER       0.24      0.50      0.33        16\n",
      "       UNKNOWN       0.90      0.44      0.60        63\n",
      "\n",
      "     micro avg       0.40      0.40      0.40       101\n",
      "     macro avg       0.34      0.33      0.30       101\n",
      "  weighted avg       0.63      0.40      0.45       101\n",
      "\n",
      "Epoch 111 Completed: 1.117095\n",
      "Epoch 112 Completed: 1.099618\n",
      "Epoch 113 Completed: 1.109932\n",
      "Epoch 114 Completed: 1.125277\n",
      "Epoch 115 Completed: 1.166844\n",
      "Epoch 116 Completed: 1.083896\n",
      "Epoch 117 Completed: 1.084813\n"
     ]
    }
   ],
   "source": [
    "#bert tokenizer:\n",
    "bert_tokenizer = BertTokenizer.from_pretrained( bert_model_path )\n",
    "\n",
    "#model:\n",
    "bert_doc_classifier=DocumentBertLSTM.from_pretrained( bert_model_path  , config=config   )\n",
    "\n",
    "bert_doc_classifier.freeze_bert_encoder()\n",
    "\n",
    "#for param in bert_doc_classifier.bert.parameters():\n",
    "#    print(param.requires_grad)\n",
    "     #= False\n",
    "\n",
    "#optimizer:\n",
    "optimizer = torch.optim.Adam(\n",
    "            bert_doc_classifier.parameters(),  #you could limit here to the lstm parameters (as opposed to using with torch.no_grad() in the DocumentBertLSTM class ) \n",
    "            weight_decay=weight_decay,\n",
    "            lr=learning_rate\n",
    "        )\n",
    "\n",
    "train( bert_doc_classifier , optimizer , bert_tokenizer, ( train_documents, train_labels )  , (dev_documents, dev_labels) , batch_size , model_directory , labels, epochs=1000 , device=device )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on Epoch 1\n"
     ]
    }
   ],
   "source": [
    "predictions=predict( bert_doc_classifier, bert_tokenizer, (dev_documents, dev_labels), 10, 1 , '', labels   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_label= torch.argmax(predictions, dim=1)  #get the predicted labels predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PAST SMOKER', 'CURRENT SMOKER', 'NON-SMOKER', 'UNKNOWN']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "   PAST SMOKER       0.00      0.00      0.00         0\n",
      "CURRENT SMOKER       0.00      0.00      0.00         0\n",
      "    NON-SMOKER       0.88      0.26      0.41        53\n",
      "       UNKNOWN       0.65      0.85      0.74        48\n",
      "\n",
      "     micro avg       0.54      0.54      0.54       101\n",
      "     macro avg       0.38      0.28      0.29       101\n",
      "  weighted avg       0.77      0.54      0.56       101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.info( f\"{metrics.classification_report(  predictions_label, dev_labels, target_names=labels   )}\"  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PAST SMOKER', 'CURRENT SMOKER', 'NON-SMOKER', 'UNKNOWN']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "   PAST SMOKER       0.00      0.00      0.00         0\n",
      "CURRENT SMOKER       0.00      0.00      0.00         0\n",
      "    NON-SMOKER       0.88      0.26      0.41        53\n",
      "       UNKNOWN       0.65      0.85      0.74        48\n",
      "\n",
      "     micro avg       0.54      0.54      0.54       101\n",
      "     macro avg       0.38      0.28      0.29       101\n",
      "  weighted avg       0.77      0.54      0.56       101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.info( metrics.classification_report(  predictions_label, dev_labels, target_names=labels     ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_label= np.argmax(predictions, axis=1)  #get the predicted labels predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3,\n",
       "        2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2,\n",
       "        2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 3,\n",
       "        3, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2826,  0.1321,  0.4736, -0.4676])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3,\n",
       "        2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2,\n",
       "        2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 3,\n",
       "        3, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solutionm, don't freeze batchnorm layers of BERT\n",
    "\n",
    "for name, param in transfer_model.named_parameters():\n",
    "    if(\"bn\" not in name):\n",
    "        param.requires_grad = False  #freeze the layers (but don't freeze the batchnorm layers) ==> we stop them from accumulating gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sys' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-aa800e9ee796>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sys' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "977146916\n",
      "HLGMC\n",
      "2878891\n",
      "022690\n",
      "01/27/1997 12:00:00 AM\n",
      "CARCINOMA OF THE COLON .\n",
      "Unsigned\n",
      "DIS\n",
      "Report Status :\n",
      "Unsigned\n",
      "Please do not go above this box important format codes are contained .\n",
      "DISCHARGE SUMMARY\n",
      "ARF32 FA\n",
      "DISCHARGE SUMMARY NAME :\n",
      "GIRRESNET , DIEDREO A\n",
      "UNIT NUMBER :\n",
      "075-71-01\n",
      "ADMISSION DATE :\n",
      "01/27/1997\n",
      "DISCHARGE DATE :\n",
      "01/31/1997\n",
      "PRINCIPAL DIAGNOSIS :\n",
      "Carcinoma of the colon .\n",
      "ASSOCIATED DIAGNOSIS :\n",
      "Urinary tract infection , and cirrhosis of the liver .\n",
      "HISTORY OF PRESENT ILLNESS :\n",
      "The patient is an 80-year-old male , who had a history of colon cancer in the past , resected approximately ten years prior to admission , history of heavy alcohol use , who presented with a two week history of poor PO intake , weight loss , and was noted to have acute on chronic Hepatitis by chemistries and question of pyelonephritis .\n",
      "He lived alone but was driven to the hospital by his son because of reported worsening and general care and deconditioning .\n",
      "Emergency Department course ; he was evaluated in the emergency room , found to be severely cachectic and jaundiced .\n",
      "He was given a liter of normal saline , along with thiamine , folate .\n",
      "An abdominal ultrasound was performed showing no stones .\n",
      "Chest x-ray revealed clear lungs and then he was admitted to Team C for management .\n",
      "PAST MEDICAL HISTORY :\n",
      "Cancer , ten years prior to admission , status post resection .\n",
      "MEDICATIONS ON ADMISSION :\n",
      "Folic acid .\n",
      "ALLERGIES :\n",
      "None .\n",
      "FAMILY HISTORY :\n",
      "Not obtained .\n",
      "SOCIAL HISTORY :\n",
      "Lives in Merca .\n",
      "Drinks ginger brandy to excess , pipe and cigar smoker for many years .\n",
      "PHYSICAL EXAMINATION :\n",
      "In general was a cachectic , jaundiced man .\n",
      "bloodpressure :\n",
      "124/60 , 97.4 , 84 , 22 for vital signs .\n",
      "head , eyes , ears , nose and throat :\n",
      "notable for abscess ulcers on the lower gums .\n",
      "He was edentulous .\n",
      "Neck was supple , lungs were clear except for some scattered mild crackles .\n",
      "Cardiac :\n",
      "tachycardic with a II / VI systolic ejection murmur .\n",
      "Belly was tender in the right upper quadrant .\n",
      "Liver edge , thickened abdominal wall was palpable .\n",
      "No inguinal nodes .\n",
      "Rectal was guaiac negative .\n",
      "On mental status exam , he was somnolent but arousable .\n",
      "Oriented to name , year , and hospital .\n",
      "Skin was jaundiced .\n",
      "LABORATORY DATA :\n",
      "Notable for a BUN and creatinine 14 and 1.8 , phosphorous of .5 , magnesium 1.2 , albumin 2.1 .\n",
      "elevated liver function tests , bilirubin of 14 direct , 17 total .\n",
      "uric acid 11.4 , alkaline phosphatase 173 , serum glutamic oxaloacetic transaminase 309 , amylase 388 .\n",
      "His urinalysis showed 10-20 granular casts and 10-20 white blood cells , 3-5 red blood cells , 5-10 whites , 3-5 white blood cells cast .\n",
      "The white blood cell was 8.5 , hematocrit 34 .\n",
      "platelet count 74 .\n",
      "5% bands on differential .\n",
      "prothrombin time 14.9 , partial thromboplastin time 35 .\n",
      "HOSPITAL COURSE AND TREATMENT :\n",
      "The patient was admitted to the Staviewordna University Of Medical Center .\n",
      "His mental status proceeded to decline as he became more sleepy and less arousable and confused .\n",
      "His Hepatitis worsened , liver failure progressed with his coagulopathy worsening .\n",
      "His renal status also decreased with a drop in urine output , became more shortness of breath as he developed some pulmonary edema .\n",
      "A head computerized tomography scan was planned to evaluate his change in mental status , but after an extensive discussion with the son , who felt that he and other family members wanted to maximize the patient 's comforts and avoid heroic measures in the event of further deterioration , plans were made to make the patient as comfortable as possible .\n",
      "He was continued on antibiotics , and oxygen , and morphine , and small amounts of Dopamine , and at 4 AM on January 31 , was pronounced dead .\n",
      "_________________________ AJO C. CUCHKOTE , M.D.\n",
      "TR :\n",
      "tfv\n",
      "DD :\n",
      "09/08/1997\n",
      "TD :\n",
      "10/13/1997 3:47\n",
      "Pcc :\n",
      "AZEL USANNE WALL , M.D.\n",
      "[ report_end ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model name '/notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "Didn't find file /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/added_tokens.json. We won't load it.\n",
      "Didn't find file /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/special_tokens_map.json. We won't load it.\n",
      "Didn't find file /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/tokenizer_config.json. We won't load it.\n",
      "loading file /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained( bert_model_path )\n",
    "\n",
    "tokenized_documents = [bert_tokenizer.tokenize(document)[:10200] for document in train_documents]  #added by AD (only take first 10200 tokens of each documents as input)\n",
    "\n",
    "for document in tokenized_documents:\n",
    "    if len(document)> bert_batch_size*510:\n",
    "        print(\"bb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3570"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7*510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
