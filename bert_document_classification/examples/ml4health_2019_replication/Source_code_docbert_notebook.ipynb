{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:99% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from base64 import b64encode\n",
    "from base64 import b64decode\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (60,60)\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "# make the Jupyter notebook use the full screen width\n",
    "display(HTML(\"<style>.container { width:99% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg_resources import resource_exists, resource_listdir, resource_string, resource_stream,resource_filename\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy\n",
    "\n",
    "#from pytorch_transformers.tokenization_bert import BertTokenizer\n",
    "#from pytorch_transformers.modeling_utils import  CONFIG_NAME\n",
    "#from pytorch_transformers.modeling_bert import BertPreTrainedModel, BertConfig, BertModel\n",
    "\n",
    "from transformers.tokenization_bert import BertTokenizer\n",
    "from transformers import CONFIG_NAME, WEIGHTS_NAME\n",
    "from transformers.modeling_bert import BertPreTrainedModel, BertConfig, BertModel\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn import LSTM\n",
    "import torch, math, logging, os\n",
    "import sys, os, time, socket\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels='PAST SMOKER, CURRENT SMOKER, NON-SMOKER, UNKNOWN'\n",
    "device='cuda:0'\n",
    "batch_size=5  #batch size of lstm\n",
    "\n",
    "\n",
    "bert_model_path= \\\n",
    "'/notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/'\n",
    "#bert_model_path='bert-base-uncased'\n",
    "bert_batch_size=7\n",
    "\n",
    "#parameters for optimizer\n",
    "weight_decay=0\n",
    "learning_rate=6e-5\n",
    "\n",
    "model_storage_directory='results_test'\n",
    "\n",
    "\n",
    "labels = [x for x in labels.split(', ')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set run specific envirorment configurations\n",
    "timestamp = time.strftime(\"run_%Y_%m_%d_%H_%M_%S\") + \"_{machine}\".format(machine=socket.gethostname())\n",
    "model_directory = os.path.join(model_storage_directory, timestamp) #directory\n",
    "os.makedirs(model_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling logging configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger()\n",
    "log.handlers.clear()\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "fh = logging.FileHandler(os.path.join(model_directory, \"log.txt\"))\n",
    "fh.setLevel(logging.INFO)\n",
    "fh.setFormatter(formatter)\n",
    "log.addHandler(fh)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "log.setLevel(logging.INFO)\n",
    "log.addHandler(ch)\n",
    "#log.info(p.format_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_n2c2_2006(partition='train'):\n",
    "    \"\"\"\n",
    "    Yields a generator of id, doc, label tuples.\n",
    "    :param partition:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert partition in ['train', 'test']\n",
    "\n",
    "    if partition == 'train':\n",
    "        with open(\"data/smokers_surrogate_%s_all_version2.xml\" % partition) as raw:\n",
    "            file = raw.read().strip()\n",
    "        \n",
    "    elif partition == 'test':\n",
    "        with open(\"data/smokers_surrogate_%s_all_groundtruth_version2.xml\" % partition) as raw:\n",
    "            file = raw.read().strip()   \n",
    "        \n",
    "    # file = resource_string('clinical_data', 'phenotyping/n2c2_2006/smokers_surrogate_%s_all_version2.xml' % partition).decode('utf-8').strip()\n",
    "    root = ET.fromstring(file)\n",
    "    ids = []\n",
    "    notes = []\n",
    "    labels = []\n",
    "    documents = root.findall(\"./RECORD\")\n",
    "    for document in documents:\n",
    "        ids.append(document.attrib['ID'])\n",
    "        notes.append(document.findall('./TEXT')[0].text)\n",
    "        labels.append(document.findall('./SMOKING')[0].attrib['STATUS'])\n",
    "\n",
    "    for id, note, label in zip(ids,notes,labels):\n",
    "        yield (id,note,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data:\n",
    "train, dev = load_n2c2_2006(partition='train'), load_n2c2_2006(partition='test')\n",
    "\n",
    "train_documents, train_labels = [],[]\n",
    "for _, text, status in train:\n",
    "    train_documents.append(text)\n",
    "    label = [0]*len(labels)\n",
    "    for idx, name in enumerate(labels):\n",
    "        if name == status:\n",
    "            label[idx] = 1\n",
    "    train_labels.append(label)\n",
    "\n",
    "dev_documents, dev_labels = [],[]\n",
    "for _, text, status in dev:\n",
    "    dev_documents.append(text)\n",
    "    label = [0]*len(labels)\n",
    "    for idx, name in enumerate(labels):\n",
    "        if name == status:\n",
    "            label[idx] = 1\n",
    "    dev_labels.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentBertLSTM(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    BERT output over document in LSTM\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert_model_config: BertConfig):\n",
    "        #super(DocumentBertLSTM, self).__init__(bert_model_config)\n",
    "        super().__init__(bert_model_config)\n",
    "        self.bert = BertModel(bert_model_config)\n",
    "        self.bert_batch_size= self.bert.config.bert_batch_size\n",
    "        self.dropout = nn.Dropout(p=bert_model_config.hidden_dropout_prob)\n",
    "        self.lstm = LSTM(bert_model_config.hidden_size,bert_model_config.hidden_size, )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=bert_model_config.hidden_dropout_prob),\n",
    "            nn.Linear(bert_model_config.hidden_size, bert_model_config.num_labels),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.init_weights()        \n",
    "\n",
    "    #input_ids, token_type_ids, attention_masks\n",
    "    def forward(self, document_batch: torch.Tensor, document_sequence_lengths: list, device='cuda'):\n",
    "\n",
    "        #contains all BERT sequences\n",
    "        #bert should output a (batch_size, num_sequences, bert_hidden_size)\n",
    "        bert_output = torch.zeros(size=(document_batch.shape[0],\n",
    "                                              min(document_batch.shape[1],self.bert_batch_size),\n",
    "                                              self.bert.config.hidden_size), dtype=torch.float, device=device)\n",
    "\n",
    "        #only pass through bert_batch_size numbers of inputs into bert.\n",
    "        #this means that we are possibly cutting off the last part of documents.\n",
    "        #use_grad = not freeze_bert\n",
    "    \n",
    "        \n",
    "        \n",
    "        for doc_id in range(document_batch.shape[0]):\n",
    "            bert_output[doc_id][:self.bert_batch_size] = self.dropout(self.bert(document_batch[doc_id][:self.bert_batch_size,0],\n",
    "                                            token_type_ids=document_batch[doc_id][:self.bert_batch_size,1],\n",
    "                                            attention_mask=document_batch[doc_id][:self.bert_batch_size,2])[1])\n",
    "\n",
    "        #lstm expects a ( num_sequences, batch_size (i.e. number of documents) , bert_hidden_size )\n",
    "        self.lstm.flatten_parameters()\n",
    "        output, (_, _) = self.lstm(bert_output.permute(1,0,2))\n",
    "        \n",
    "        #print(bert_output.requires_grad)\n",
    "        #print(output.requires_grad)\n",
    "\n",
    "        last_layer = output[-1]\n",
    "        #print(\"Last LSTM layer shape:\",last_layer.shape)\n",
    "\n",
    "        prediction = self.classifier(last_layer)\n",
    "        #print(\"Prediction Shape\", prediction.shape)\n",
    "        assert prediction.shape[0] == document_batch.shape[0]\n",
    "        return prediction\n",
    "    \n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/\n",
      "BertConfig {\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bert_batch_size\": 7,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists( bert_model_path  ):\n",
    "    if os.path.exists(os.path.join(bert_model_path, CONFIG_NAME)):\n",
    "        print( f\"loading {bert_model_path}\" )\n",
    "        config = BertConfig.from_json_file(os.path.join(bert_model_path, CONFIG_NAME))\n",
    "    elif os.path.exists(os.path.join(bert_model_path, 'bert_config.json')):\n",
    "        print( f\"loading {bert_model_path}\" )\n",
    "        config = BertConfig.from_json_file(os.path.join(bert_model_path, 'bert_config.json'))\n",
    "    else:\n",
    "        raise ValueError(\"Cannot find a configuration for the BERT based model you are attempting to load.\")\n",
    "else:\n",
    "    config = BertConfig.from_pretrained(bert_model_path )\n",
    "\n",
    "\n",
    "config.__setattr__( 'num_labels', len( labels ) )\n",
    "config.__setattr__( 'bert_batch_size', bert_batch_size )\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Helper function for encoding documents (tokenization of sentences, word2id), and saving checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_documents(documents: list, tokenizer: BertTokenizer, max_input_length=512):\n",
    "    \"\"\"\n",
    "    Returns a len(documents) * max_sequences_per_document * 3 * 512 tensor where len(documents) is the batch\n",
    "    dimension and the others encode bert input.\n",
    "\n",
    "    This is the input to any of the document bert architectures.\n",
    "\n",
    "    :param documents: a list of text documents\n",
    "    :param tokenizer: the sentence piece bert tokenizer\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tokenized_documents = [tokenizer.tokenize(document)[:10200] for document in documents]  #added by AD (only take first 10200 tokens of each documents as input)\n",
    "    max_sequences_per_document = math.ceil(max(len(x)/(max_input_length-2) for x in tokenized_documents))\n",
    "    assert max_sequences_per_document <= 20, \"Your document is to large, arbitrary size when writing\"\n",
    "\n",
    "    output = torch.zeros(size=(len(documents), max_sequences_per_document, 3, 512), dtype=torch.long)\n",
    "    document_seq_lengths = [] #number of sequence generated per document\n",
    "    #Need to use 510 to account for 2 padding tokens\n",
    "    for doc_index, tokenized_document in enumerate(tokenized_documents):\n",
    "        max_seq_index = 0\n",
    "        for seq_index, i in enumerate(range(0, len(tokenized_document), (max_input_length-2))):\n",
    "            raw_tokens = tokenized_document[i:i+(max_input_length-2)]\n",
    "            tokens = []\n",
    "            input_type_ids = []\n",
    "\n",
    "            tokens.append(\"[CLS]\")\n",
    "            input_type_ids.append(0)\n",
    "            for token in raw_tokens:\n",
    "                tokens.append(token)\n",
    "                input_type_ids.append(0)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            input_type_ids.append(0)\n",
    "\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            attention_masks = [1] * len(input_ids)\n",
    "\n",
    "            while len(input_ids) < max_input_length:\n",
    "                input_ids.append(0)\n",
    "                input_type_ids.append(0)\n",
    "                attention_masks.append(0)\n",
    "\n",
    "            assert len(input_ids) == 512 and len(attention_masks) == 512 and len(input_type_ids) == 512\n",
    "\n",
    "            #we are ready to rumble\n",
    "            output[doc_index][seq_index] = torch.cat((torch.LongTensor(input_ids).unsqueeze(0),\n",
    "                                                           torch.LongTensor(input_type_ids).unsqueeze(0),\n",
    "                                                           torch.LongTensor(attention_masks).unsqueeze(0)),\n",
    "                                                          dim=0)\n",
    "            max_seq_index = seq_index\n",
    "        document_seq_lengths.append(max_seq_index+1)\n",
    "    return output, torch.LongTensor(document_seq_lengths)\n",
    "\n",
    "#helper function to save checkpoints\n",
    "\n",
    "def save_checkpoint( model: DocumentBertLSTM, tokenizer: BertTokenizer , checkpoint_path: str):\n",
    "    \"\"\"\n",
    "    Saves an instance of the current model to the specified path.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        os.mkdir(checkpoint_path)\n",
    "    else:\n",
    "        raise ValueError(\"Attempting to save checkpoint to an existing directory\")\n",
    "    log.info(\"Saving checkpoint: %s\" % checkpoint_path )\n",
    "\n",
    "    #save finetune parameters\n",
    "    net = model\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        net = model.module\n",
    "    torch.save(net.state_dict(), os.path.join(checkpoint_path, WEIGHTS_NAME))\n",
    "    #save configurations\n",
    "    net.config.to_json_file(os.path.join(checkpoint_path, CONFIG_NAME))\n",
    "    #save exact vocabulary utilized\n",
    "    tokenizer.save_vocabulary(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Train and predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( model: DocumentBertLSTM , optimizer: torch.optim.Adam , tokenizer: BertTokenizer, train: tuple, dev: tuple, batch_size: int , output_path:str, labels:list, epochs=10 , device='cuda:0'  ):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    train_documents, train_labels = train  #train is tuple\n",
    "    dev_documents, dev_labels = dev\n",
    "    \n",
    "    document_representations, document_sequence_lengths = encode_documents(train_documents, bert_tokenizer)\n",
    "\n",
    "    correct_output = torch.FloatTensor(train_labels)\n",
    "    \n",
    "    assert document_representations.shape[0] == correct_output.shape[0]\n",
    "    \n",
    "    #if torch.cuda.device_count()>1:\n",
    "        #model=torch.nn.DataParallel( model )\n",
    "    \n",
    "    model.to( device=device )\n",
    "    \n",
    "    #get the loss function\n",
    "    \n",
    "    loss_weight = ((correct_output.shape[0] / torch.sum(correct_output, dim=0))-1).to(device=device)\n",
    "    loss_function = torch.nn.BCEWithLogitsLoss(pos_weight=loss_weight)\n",
    "    #loss_function=torch.nn.CrossEntropyLoss(weight=loss_weight)\n",
    "    \n",
    "    for epoch in range( 1, epochs+1  ):\n",
    "        # shuffle\n",
    "        permutation = torch.randperm(document_representations.shape[0])\n",
    "        document_representations = document_representations[permutation]\n",
    "        document_sequence_lengths = document_sequence_lengths[permutation]\n",
    "        correct_output = correct_output[permutation]\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        for i in range(0, document_representations.shape[0], batch_size  ):\n",
    "\n",
    "            batch_document_tensors = document_representations[i:i + batch_size  ].to(device )\n",
    "            batch_document_sequence_lengths= document_sequence_lengths[i:i+ batch_size  ]\n",
    "            #self.log.info(batch_document_tensors.shape)\n",
    "            batch_predictions=model( batch_document_tensors, batch_document_sequence_lengths, device=device )  #we freeze bert\n",
    "            batch_correct_output = correct_output[i:i + batch_size ].to( device=device )\n",
    "            loss = loss_function(batch_predictions, batch_correct_output)\n",
    "            epoch_loss += float(loss.item())\n",
    "            #self.log.info(batch_predictions)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss /= int(document_representations.shape[0] / batch_size )  # divide by number of batches per epoch\n",
    "\n",
    "        log.info('Epoch %i Completed: %f' % (epoch, epoch_loss))\n",
    "    \n",
    "        if epoch % 250 == 0:\n",
    "            save_checkpoint( model, tokenizer, os.path.join( output_path , \"checkpoint_%s\" % epoch ) )\n",
    "\n",
    "        # evaluate on development data\n",
    "        if epoch % 10 == 0:\n",
    "            predict( model, tokenizer, (dev_documents, dev_labels), batch_size , epoch, output_path, labels, device=device )\n",
    "                \n",
    "        #make sure that it is back on train...\n",
    "        \n",
    "\n",
    "def predict( model: DocumentBertLSTM, tokenizer: BertTokenizer, data, batch_size:int, epoch:int, output_path:str, labels:list, threshold=0, device='cuda:0' ):\n",
    "    \"\"\"\n",
    "    A tuple containing\n",
    "    :param data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    document_representations = None\n",
    "    document_sequence_lengths = None\n",
    "    correct_output = None\n",
    "    if isinstance(data, list):\n",
    "        document_representations, document_sequence_lengths = encode_documents(data, tokenizer)\n",
    "    if isinstance(data, tuple) and len(data) == 2:\n",
    "        log.info('Evaluating on Epoch %i' % (epoch))\n",
    "        document_representations, document_sequence_lengths = encode_documents(data[0], tokenizer)\n",
    "        correct_output = torch.FloatTensor(data[1]).transpose(0,1)\n",
    "        assert labels is not None\n",
    "\n",
    "    model.to(device=device )\n",
    "    \n",
    "    #put in eval mode:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = torch.empty((document_representations.shape[0], len(labels )  ))\n",
    "        for i in range(0, document_representations.shape[0], batch_size  ):\n",
    "            batch_document_tensors = document_representations[i:i + batch_size  ].to(device=device)\n",
    "            batch_document_sequence_lengths= document_sequence_lengths[i:i+batch_size  ]\n",
    "\n",
    "            prediction = model(batch_document_tensors, batch_document_sequence_lengths,device=device  )\n",
    "            predictions[i:i + batch_size  ] = prediction\n",
    "\n",
    "    for r in range(0, predictions.shape[0]):\n",
    "        for c in range(0, predictions.shape[1]):\n",
    "            if predictions[r][c] > threshold:\n",
    "                predictions[r][c] = 1\n",
    "            else:\n",
    "                predictions[r][c] = 0\n",
    "    predictions = predictions.transpose(0, 1)\n",
    "\n",
    "    if correct_output is None:\n",
    "        return predictions.cpu()\n",
    "    else:\n",
    "        assert correct_output.shape == predictions.shape\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        fmeasures = []\n",
    "\n",
    "        for label_idx in range(predictions.shape[0]):\n",
    "            correct = correct_output[label_idx].cpu().view(-1).numpy()\n",
    "            predicted = predictions[label_idx].cpu().view(-1).numpy()\n",
    "            present_f1_score = f1_score(correct, predicted, average='binary', pos_label=1)\n",
    "            present_precision_score = precision_score(correct, predicted, average='binary', pos_label=1)\n",
    "            present_recall_score = recall_score(correct, predicted, average='binary', pos_label=1)\n",
    "\n",
    "            precisions.append(present_precision_score)\n",
    "            recalls.append(present_recall_score)\n",
    "            fmeasures.append(present_f1_score)\n",
    "            logging.info('F1\\t%s\\t%f' % (labels[label_idx], present_f1_score))\n",
    "\n",
    "        micro_f1 = f1_score(correct_output.reshape(-1).numpy(), predictions.reshape(-1).numpy(), average='micro')\n",
    "        macro_f1 = f1_score(correct_output.reshape(-1).numpy(), predictions.reshape(-1).numpy(), average='macro')\n",
    "\n",
    "        #if 'use_tensorboard' in self.args and self.args['use_tensorboard']:\n",
    "        #    for label_idx in range(predictions.shape[0]):\n",
    "        #        self.tensorboard_writer.add_scalar('Precision/%s/Test' % self.args['labels'][label_idx].replace(\" \", \"_\"), precisions[label_idx], self.epoch)\n",
    "        #        self.tensorboard_writer.add_scalar('Recall/%s/Test' % self.args['labels'][label_idx].replace(\" \", \"_\"), recalls[label_idx], self.epoch)\n",
    "        #        self.tensorboard_writer.add_scalar('F1/%s/Test' % self.args['labels'][label_idx].replace(\" \", \"_\"), fmeasures[label_idx], self.epoch)\n",
    "        #    self.tensorboard_writer.add_scalar('Micro-F1/Test', micro_f1, self.epoch)\n",
    "        #    self.tensorboard_writer.add_scalar('Macro-F1/Test', macro_f1, self.epoch)\n",
    "\n",
    "        with open(os.path.join( output_path , \"eval_%s.csv\" % epoch), 'w') as eval_results:\n",
    "            eval_results.write('Metric\\t' + '\\t'.join([ labels[label_idx] for label_idx in range(predictions.shape[0])]) +'\\n' )\n",
    "            eval_results.write('Precision\\t' + '\\t'.join([str(precisions[label_idx]) for label_idx in range(predictions.shape[0])]) + '\\n' )\n",
    "            eval_results.write('Recall\\t' + '\\t'.join([str(recalls[label_idx]) for label_idx in range(predictions.shape[0])]) + '\\n' )\n",
    "            eval_results.write('F1\\t' + '\\t'.join([ str(fmeasures[label_idx]) for label_idx in range(predictions.shape[0])]) + '\\n' )\n",
    "            eval_results.write('Micro-F1\\t' + str(micro_f1) + '\\n' )\n",
    "            eval_results.write('Macro-F1\\t' + str(macro_f1) + '\\n' )\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model name '/notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "Didn't find file /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/added_tokens.json. We won't load it.\n",
      "Didn't find file /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/special_tokens_map.json. We won't load it.\n",
      "Didn't find file /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/tokenizer_config.json. We won't load it.\n",
      "loading file /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading weights file /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_BERT/bert_document_classification/examples/ml4health_2019_replication/clinicalBERT/pretrained_bert_tf/biobert_pretrain_output_all_notes_150000/pytorch_model.bin\n",
      "Weights of DocumentBertLSTM not initialized from pretrained model: ['lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'classifier.1.weight', 'classifier.1.bias']\n",
      "Weights from pretrained model not used in DocumentBertLSTM: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "Epoch 1 Completed: 1.078455\n",
      "Epoch 2 Completed: 1.049107\n",
      "Epoch 3 Completed: 1.009635\n",
      "Epoch 4 Completed: 1.010227\n",
      "Epoch 5 Completed: 0.993543\n",
      "Epoch 6 Completed: 0.975204\n",
      "Epoch 7 Completed: 0.985157\n",
      "Epoch 8 Completed: 0.981220\n",
      "Epoch 9 Completed: 0.986926\n",
      "Epoch 10 Completed: 0.984388\n",
      "Evaluating on Epoch 10\n",
      "F1\tPAST SMOKER\t0.281250\n",
      "F1\tCURRENT SMOKER\t0.214286\n",
      "F1\tNON-SMOKER\t0.388889\n",
      "F1\tUNKNOWN\t0.738739\n",
      "Epoch 11 Completed: 0.978683\n",
      "Epoch 12 Completed: 0.974414\n",
      "Epoch 13 Completed: 0.961251\n",
      "Epoch 14 Completed: 0.976493\n",
      "Epoch 15 Completed: 0.966096\n",
      "Epoch 16 Completed: 0.971620\n",
      "Epoch 17 Completed: 0.957736\n",
      "Epoch 18 Completed: 0.951485\n",
      "Epoch 19 Completed: 0.952967\n",
      "Epoch 20 Completed: 0.955668\n",
      "Evaluating on Epoch 20\n",
      "F1\tPAST SMOKER\t0.235294\n",
      "F1\tCURRENT SMOKER\t0.218750\n",
      "F1\tNON-SMOKER\t0.400000\n",
      "F1\tUNKNOWN\t0.745763\n",
      "Epoch 21 Completed: 0.957469\n",
      "Epoch 22 Completed: 0.959757\n",
      "Epoch 23 Completed: 0.944064\n",
      "Epoch 24 Completed: 0.953423\n",
      "Epoch 25 Completed: 0.961523\n",
      "Epoch 26 Completed: 0.932679\n",
      "Epoch 27 Completed: 0.941754\n",
      "Epoch 28 Completed: 0.942691\n",
      "Epoch 29 Completed: 0.938384\n",
      "Epoch 30 Completed: 0.920397\n",
      "Evaluating on Epoch 30\n",
      "F1\tPAST SMOKER\t0.262295\n",
      "F1\tCURRENT SMOKER\t0.162162\n",
      "F1\tNON-SMOKER\t0.384615\n",
      "F1\tUNKNOWN\t0.691589\n",
      "Epoch 31 Completed: 0.939860\n",
      "Epoch 32 Completed: 0.929614\n",
      "Epoch 33 Completed: 0.925684\n",
      "Epoch 34 Completed: 0.920581\n",
      "Epoch 35 Completed: 0.928178\n",
      "Epoch 36 Completed: 0.910624\n",
      "Epoch 37 Completed: 0.931880\n",
      "Epoch 38 Completed: 0.925703\n",
      "Epoch 39 Completed: 0.901216\n",
      "Epoch 40 Completed: 0.904200\n",
      "Evaluating on Epoch 40\n",
      "F1\tPAST SMOKER\t0.243243\n",
      "F1\tCURRENT SMOKER\t0.238095\n",
      "F1\tNON-SMOKER\t0.376471\n",
      "F1\tUNKNOWN\t0.568421\n",
      "Epoch 41 Completed: 0.927546\n",
      "Epoch 42 Completed: 0.900053\n",
      "Epoch 43 Completed: 0.887861\n",
      "Epoch 44 Completed: 0.906499\n",
      "Epoch 45 Completed: 0.893759\n",
      "Epoch 46 Completed: 0.932956\n",
      "Epoch 47 Completed: 0.880509\n",
      "Epoch 48 Completed: 0.882125\n",
      "Epoch 49 Completed: 0.892997\n",
      "Epoch 50 Completed: 0.869407\n",
      "Evaluating on Epoch 50\n",
      "F1\tPAST SMOKER\t0.281250\n",
      "F1\tCURRENT SMOKER\t0.205882\n",
      "F1\tNON-SMOKER\t0.405063\n",
      "F1\tUNKNOWN\t0.647619\n",
      "Epoch 51 Completed: 0.914686\n",
      "Epoch 52 Completed: 0.884331\n",
      "Epoch 53 Completed: 0.874507\n",
      "Epoch 54 Completed: 0.885184\n",
      "Epoch 55 Completed: 0.895946\n",
      "Epoch 56 Completed: 0.874262\n",
      "Epoch 57 Completed: 0.882692\n",
      "Epoch 58 Completed: 0.872884\n",
      "Epoch 59 Completed: 0.898473\n",
      "Epoch 60 Completed: 0.872997\n",
      "Evaluating on Epoch 60\n",
      "F1\tPAST SMOKER\t0.250000\n",
      "F1\tCURRENT SMOKER\t0.192308\n",
      "F1\tNON-SMOKER\t0.468750\n",
      "F1\tUNKNOWN\t0.786885\n",
      "Epoch 61 Completed: 0.861763\n",
      "Epoch 62 Completed: 0.888788\n",
      "Epoch 63 Completed: 0.873639\n",
      "Epoch 64 Completed: 0.890151\n",
      "Epoch 65 Completed: 0.869388\n",
      "Epoch 66 Completed: 0.869445\n",
      "Epoch 67 Completed: 0.856283\n",
      "Epoch 68 Completed: 0.854370\n",
      "Epoch 69 Completed: 0.869375\n",
      "Epoch 70 Completed: 0.868151\n",
      "Evaluating on Epoch 70\n",
      "F1\tPAST SMOKER\t0.259259\n",
      "F1\tCURRENT SMOKER\t0.179487\n",
      "F1\tNON-SMOKER\t0.400000\n",
      "F1\tUNKNOWN\t0.716981\n",
      "Epoch 71 Completed: 0.873749\n",
      "Epoch 72 Completed: 0.860580\n",
      "Epoch 73 Completed: 0.880699\n",
      "Epoch 74 Completed: 0.862912\n",
      "Epoch 75 Completed: 0.887965\n",
      "Epoch 76 Completed: 0.868353\n",
      "Epoch 77 Completed: 0.849899\n",
      "Epoch 78 Completed: 0.863709\n",
      "Epoch 79 Completed: 0.859739\n",
      "Epoch 80 Completed: 0.857692\n",
      "Evaluating on Epoch 80\n",
      "F1\tPAST SMOKER\t0.269231\n",
      "F1\tCURRENT SMOKER\t0.213333\n",
      "F1\tNON-SMOKER\t0.400000\n",
      "F1\tUNKNOWN\t0.692308\n",
      "Epoch 81 Completed: 0.863314\n",
      "Epoch 82 Completed: 0.854633\n",
      "Epoch 83 Completed: 0.844148\n",
      "Epoch 84 Completed: 0.854316\n",
      "Epoch 85 Completed: 0.845529\n",
      "Epoch 86 Completed: 0.837138\n",
      "Epoch 87 Completed: 0.840595\n",
      "Epoch 88 Completed: 0.864238\n"
     ]
    }
   ],
   "source": [
    "#bert tokenizer:\n",
    "bert_tokenizer = BertTokenizer.from_pretrained( bert_model_path )\n",
    "\n",
    "#model:\n",
    "bert_doc_classifier=DocumentBertLSTM.from_pretrained( bert_model_path  , config=config   )\n",
    "\n",
    "bert_doc_classifier.freeze_bert_encoder()\n",
    "\n",
    "#for param in bert_doc_classifier.bert.parameters():\n",
    "#    print(param.requires_grad)\n",
    "     #= False\n",
    "\n",
    "#optimizer:\n",
    "optimizer = torch.optim.Adam(\n",
    "            bert_doc_classifier.parameters(),  #you could limit here to the lstm parameters (as opposed to using with torch.no_grad() in the DocumentBertLSTM class ) \n",
    "            weight_decay=weight_decay,\n",
    "            lr=learning_rate\n",
    "        )\n",
    "\n",
    "train( bert_doc_classifier , optimizer , bert_tokenizer, ( train_documents, train_labels )  , (dev_documents, dev_labels) , batch_size , model_directory , labels, epochs=250 , device=device )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_output = torch.FloatTensor(train_labels)\n",
    "loss_weight = ((correct_output.shape[0] / torch.sum(correct_output, dim=0) )-1).to(device=device)\n",
    "loss_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(correct_output.shape[0] / torch.sum(correct_output, dim=0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(correct_output, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
